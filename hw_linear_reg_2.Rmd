---
title: "Homework 02"
author: "yourname"
date: "Septemeber 16, 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

```{r}
library("dplyr")
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.

1. In R, check the dataset and clean any unusually coded data.

```{r}
heights <- na.omit(heights)
```

2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model
as average earnings for people with average height?

```{r}

earn.mean <- heights$earn - mean(heights$earn)
height.mean <- heights$height- mean(heights$height)
earn.height <- lm(earn~ height.mean, data = heights)
summary(earn.height)

new.heights <- data.frame(earn = earn.mean,height = height.mean)
coefficients(earn.height)
```




```{r}
new.earn.height <- lm(earn ~ height, data = new.heights)
coefficients(new.earn.height)
summary(new.earn.height)
```

3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.


4. Interpret all model coefficients.
```{r}
# sex is either 1 or 2, sex -1 means 0 is male and 1 is female

earning.model1<- lm(earn ~ height + (sex-1),data = heights)
display(earning.model1)
```

```{r}
earning.model2 <- lm(log(earn+1)~ height + (sex-1), data = heights)
summary(earning.model2)
```
For earning model 2:
The intercept is the predicted log earnings if height and male both equal zero. Because heights are never close to zero, the intercept has no direct interpretation.

. The coefficient for height is the predicted difference in log earnings corresponding to a 1-inch difference in height, if male equals zero. Thus, the estimated predictive difference per inch of height is 16%. The estimate is more than 2 standard errors from zero, indicating that the data are consistent with a positive predictive difference also.

. The coefficient for sex is the predicted difference in log earnings between women and men, if height equals 0. Heights are never close to zero, and so the coefficient for male has no direct interpretation in this model.

```{r}
z.height <- (heights$height-mean(heights$height))/(2*sd(heights$height))
z.sex <- ((heights$sex-1)-mean(heights$sex-1))/(2*sd(heights$sex-1))
earning.model3 <- lm(earn ~ z.height + z.sex, data = heights)
summary(earning.model3)
```
For earning model 3:
tHE INTERCEPT means 20014.86 when z.height and z.sex are both 0. It means standarizing value can give more true value about prediction.
One more inch means 4190 increase in earning
female has 10913.16 more earning than male's.
```{r}
earning.model4 <- lm(log(earn+1)~ z.height + sex,data = heights)
display(earning.model4)

```
For earning medel 4
The intercept is the predicted log earnings if z. height and male both equal zero. Thus, a 66.9-inch tall woman is predicted to have log earnings of 11.04, and thus earnings of exp(11.04) = 62317.
. The coefficient for z.height is the predicted difference in log earnings corresponding to a 1 standard-deviation difference in height, if male equals zero. Thus, the estimated predictive difference for a 3.8-inch increase in height is 71% for women.


5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
z.heights <- data.frame(earn = heights$earn, height = z.height,sex = z.sex)
confint(earning.model3, level = 0.95)

```


### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
nox.mort <- lm(mort~nox,data = pollution)
summary(nox.mort)
```


```{r}
ggplot(data = pollution)+
  geom_point(mapping = aes(x = nox, y = mort))+
  geom_abline(mapping = aes(x = nox, y= mort),slope = -0.1039,intercept = 942.7115 )

plot(nox.mort,which = 1)

```
It does not fit that well.

2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}
log.nox.mort <- lm(log(mort)~log(nox),data = pollution)
display(log.nox.mort)
```


```{r}
ggplot(data = pollution)+
  geom_point(mapping = aes(x = log(nox), y = log(mort)))+
  geom_abline(mapping = aes(x = log(nox), y= log(mort)),slope = 0.02,intercept = 6.81)
plot(log.nox.mort,which = 1)
```
The log make linear regression fit data better 
3. Interpret the slope coefficient from the model you chose in 2.

For each 1% difference in nox, the predicted difference in mort is 0.02%.

4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
confint(object = log.nox.mort,level = 0.99 )
```

5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when
helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
#z.so2 <- (pollution$so2 - mean(pollution$so2))/(2*sd(pollution$so2))
nsh.mort<- lm(log(mort)~ log(nox)+ log(so2) +log(hc),data = pollution)
summary(nsh.mort)
ggplot(data= pollution)+
  geom_smooth(mapping = aes(x = log(nox)+ log(so2) +log(hc), y= log(mort)))+
  geom_point(mapping = aes(x = log(nox)+ log(so2) +log(hc), y= log(mort)))
```
For each 1% difference in nox, the predicted difference in mort is 0.059%.
For each 1% difference in so2, the predicted difference in mort is 0.014%.
For each 1% difference in hc, the predicted difference in mort is 0-0.061%.


6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
nsh.mort2 <- lm(log(mort)~ log(nox)+ log(so2) +log(hc),data = pollution[1:30,])
display(nsh.mort2)

predict(nsh.mort2, newdata = pollution[31:60,])

```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r}
teengamb.model <- lm(gamble~sex+status+income,data = teengamb)
display(teengamb.model)
plot(teengamb.model,which =1)

```

```{r}
z.income = (teengamb$income - mean(teengamb$income ))/(2*sd(teengamb$income ))
z.status = (teengamb$status - mean(teengamb$status))/(2*sd(teengamb$status))
mean.sex = teengamb$sex-mean(teengamb$sex)
```

```{r}
teengamb.mode2 <- lm(log(gamble+1)~(mean.sex + z.status+log(income)), data = teengamb)

display(teengamb.mode2)
 
plot(teengamb.mode2,which =1)
```

R-square value means this model does not work that good.

```{r}
teengamb.mode3 <- lm(log(gamble+1)~log(sex+1)+log(status)+log(income),data = teengamb)
display(teengamb.mode3)
plot(teengamb.mode3,which =1 )
```
R-square value means this model does not work that good compare to orignial model.

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}

round(confint(teengamb.mode2,level = 0.95),3)
```
The range of [0.056, 1.512] is the 95% confidence interval within intercept. From this range, we know it does not cross zero. So this means it's significant.

The range of [-2.110, -0.475] is the 95% confidence interval within the vlue of centering sex. From this range, we know it does not cross zero. So this means the it's significant.

The range of [-0.586, 1.084] is the 95% confidence interval within the value of standarizing value. From this range, we know it crosses zero. So this means it's not significant.

The range of [0.430, 1.434] is the 95% confidence interval within log of income. From this range, we know it does not cross zero. So this means it's significant.

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
average.male <- data.frame(sex=0, status = mean(teengamb$status),income = mean(teengamb$verbal), verbal = mean(teengamb$verbal))

predict(teengamb.model, average.male, interval = "prediction")

max.male <- data.frame(sex=0, status = max(teengamb$status),income = max(teengamb$verbal), verbal = max(teengamb$verbal))
predict(teengamb.model, max.male, interval = "prediction")
```
The max Male has wider CI than average male. In my opinion, this result is expected because max value is not a normal value. Use this calue to fit the model will cause the confidence interval become higher and wider.
### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
sat.model1 <- lm(total~expend+ratio+salary,data = sat)
display(sat.model1)

ratio.mean = sat$ratio - mean(sat$ratio)
z.expend = (sat$expend - mean(sat$expend)) / 2*sd(sat$expend)
z.salary = (sat$salary -mean(sat$salary))/2*sd(sat$salary)
sat.model2 <- lm(total~ z.expend+ratio.mean+ z.salary,data = sat)
display(sat.model2)
sat.model3 <- lm(log(total)~ log(expend) + log(ratio)+log(salary),data= sat)
display(sat.model3)

```
The last two models do not work that well.
The coefficients are not significant except the intercept. 

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
round(confint(sat.model3, level = 0.98),3)
```
The range of [6.734, 8.232] is the 98% confidence interval within intercept. From this range, we know it does not cross zero. So this means it's significant.

The range of [-0.232, 0.432] is the 98% confidence interval within the log of expend. From this range, we know it does not cross zero. So this means the it's significant.

The range of [-0.175, 0.428] is the 98% confidence interval within the log of ratio. From this range, we know it crosses zero. So this means it's not significant.

The range of [-0.175, 0.428] is the 98% confidence interval within log of salary. From this range, we know it does cross zero. So this means it's not significant.


3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
taker.model1<-lm(formula = total ~ expend + ratio + salary + takers, data = sat)
display(taker.model1)
```

```{r}
takers.mean <- sat$takers - mean(sat$takers)
takers.model2 <- lm(total~ z.expend+ratio.mean+ z.salary + takers.mean,data = sat)
display(takers.model2)
```

```{r}
takers.model3 <- lm(log(total)~ log(expend) + log(ratio)+log(salary) + log(takers),data= sat)
display(takers.model3)
```
From the r-square value, the all three models are better than the original ones.

The log one is better than previous two.

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$
This is the simplest method which compare two parties 's money directly. 
Advantage: it can see difference of two parties in different district directly.
disadvantage: only number, no other way like proportin to search about these data 

* The ratio, $D_i/R_i$
Advantage: we know above 1, D get more money. We can understand this easier.
Disadvantage: 

* The difference on the logarithmic scale, $log D_i-log R_i$ 
advantage: the number after log become small and easier to calculate


* The relative proportion, $D_i/(D_i+R_i)$.
know D's part is how many percent of whole part.
Advantage: there is a comparsion to D's part to whole part. If this number is high means more money


### Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?


3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.


5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.


6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

